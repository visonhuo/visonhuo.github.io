---
title: "缓存更新模式"
date: 2022-03-03T16:48:49+08:00
lastmod: 2022-03-03T16:48:49+08:00
draft: false
author: visonhuo
description: "缓存是常用的性能优化手段，其本质是创建真实数据的高效可读副本，为了保证副本与真实数据的一致性，当真实数据更新时，我们同样需要对缓存进行更新，本文中将介绍缓存更新的一些常见模式。"
featuredImage: "/images/feature/architecture_design/cache_update_patterns.png"
featuredImagePreview: "/images/feature/architecture_design/cache_update_patterns.png"
categories: ["架构设计"]
tags: ["缓存"]
---
<!--more-->

## 前言
在计算机领域中，**缓存设计** 是一个相当重要的概念。那为什么我们需要缓存？

在有些场景中，我们需要 **平衡处理速度和 IO 读取速度**，比如 CPU 处理磁盘数据的场景，因为 CPU 的处理速度比磁盘读取的速度要快上好几个数量级，
此时便有了内存的存在，系统将一部分磁盘数据先加载到内存，CPU 则直接与内存交互以此来提高处理效率。而后的 L1，L2，L3 级缓存同样也是类似的思想。

所以缓存的核心工作在于 **提高 I/O 效率**，将部分常访问的数据从 I/O 效率较低的设备中，拷贝到 I/O 效率更高的介质中，以此来提升整体的性能。

{{< admonition type=tip title="I/O 效率是相对的" open=open >}}
这里我们需要理解，**介质的 I/O 效率是相对的**。比如说磁盘对于内存来说，是性能更低的 I/O 设备，
但本地磁盘对于网络 I/O 来说又是性能更高的存在。

所以对于磁盘来说，内存可以作为其缓存介质；对于网络 I/O 来说，磁盘可以其缓存介质。
充分理解这一点可以拓宽我们对缓存的认识，而别将其局限于内存的场景。
不要将缓存局限于磁盘和内存的场景，我们会有一个不一样的视野。
{{< /admonition >}}

综上所述，缓存本质上是底层存储数据的副本，在其提升读取效率的同时也带来了一致性相关的问题。
当底层数据被更新时，我们需要同时更新缓存数据，否则就会出现数据不一致的情况。

在本文中，将介绍一些常见的缓存更新模式，主要包括如下内容：
- [x] 缓存更新模式

- [x] 缓存更新策略

- [x] 基于 Binlog 的缓存更新方案

## 缓存更新模式
在本节中，将介绍以下 3 种常见的缓存更新模式：

- Cache Aside 模式；
  
- Read/Write Through 模式；

- Write Behind Caching 模式；

其中 Read/Write Through 其实是两种模式，但是因为其思想较为相近，故将其放在同一小节中。

### Cache Aside 模式
该模式是日常开发中最常使用的模式，在该模式下对缓存存在三种操作：

- **缓存失效** ：应用程序先从 cache 中取数据，如果没有得到，则从数据存储中获取，成功后将值放到缓存中，并将数据返回；

- **缓存命中** ：应用程序从 cache 中取数据，若命中缓存则直接返回数据；

- **数据更新** ：将数据更新到数据存储中，等操作成功之后再让缓存失效;

{{< image src="https://i.bmp.ovh/imgs/2022/03/bb9825323e52ee3e.png" width="660" caption="缓存失效" >}}

{{< image src="https://i.bmp.ovh/imgs/2022/03/6baac2fcd0c44975.png" width="660" caption="数据更新" >}}

其中的关键操作在于：**当数据更新时，先更新存储，再清理缓存，** 关于先操作数据存储还是缓存的讨论，
以及该方案存在的问题，我们将会放在后面的小节中讨论。

### Read/Write Through 模式
在 Cache Aside 模式中，应用层需要与两个数据组件进行交互，**底层存储 + 缓存组件**。
所以无论是在进行更新操作还是读取操作时，在应用层中，我们都需要与这两个组件进行交互。

而在 **Read/Write Through 模式** 中，则是直接把更新、读取底层存储的操作交由一个数据代理层进行处理。
对于应用层来说，只需要面对这个单一数据代理层来进行操作，由这个代理层组件来维护缓存的读取、更新，以此来简化应用层代码。

{{< image src="https://i.bmp.ovh/imgs/2022/03/d0c2a6fd65d53de5.png" width="450" caption="Read/Write Through模式" >}}

- **Read Through 模式** : 当缓存未命中时，直接由代理层加载数据到缓存中，并将值返回给应用层；

- **Write Through 模式** : 更新数据时，若缓存命中，则先更新缓存，然后再将该值刷入底层存储中，整个过程对应用层是透明的；

这两种模式最核心的考虑就是 **简化应用层的处理逻辑**，由专门的数据代理来处理缓存相关问题。
但是 Write Through 双写的方式存在一些细微的问题，我们会在稍后进行讨论。

{{< admonition type=tip title="代理层 != 代理组件" open=open >}}
以上所说的代理层并不一定是额外的代理组件，也可以只是应用中的一个缓存层。
比如 Spring Cache 代理注解，对于应用层代码来说，缓存的更新/删除并不需要调用方参与，
完全由 Spring Cache 代理执行了，也是符合 Read Through 模式思想的。
```Java
import org.springframework.cache.annotation.Cacheable;
import org.springframework.stereotype.Component;

@Component
public class MathService {
    @Cacheable("piDecimals")
    public int computePiDecimal(int i) {
        // ...
    }
}
```
{{< /admonition >}}

### Write Behind Caching 模式
Write Behind Caching 模式，该模式的核心思想在于，**当数据更新时，先更新缓存，但不会立即更新底层存储，稍后缓存会被异步批量地更新到底层存储中。**

这种方式的好处在于 **减少 I/O 操作（减少系统调用的次数）**，写入更多数据以此来提升 I/O 性能。
因为是先更新缓存，该模式还可以合并对于同个数据的多次操作。在并发请求很高的情况下，性能的提高是相当可观的。
比如 Linux 文件系统中的 Page Cache 算法，当我们进行文件写入时，就是先更新缓存数据，
操作系统会等待合适的时机将 Page Cache 刷回到磁盘，就是使用了类似的思想。

但该模式的缺点同样明显，因为数据没有立即落盘，如果此时系统宕机，则存在丢失数据的风险，此处需要我们在 **持久化和高性能之间做权衡**。其流程如下图所示:

{{< image src="https://i.bmp.ovh/imgs/2022/03/77336dac8ea8e1ea.png" width="550" caption="Write Behind Caching模式" >}}

当存在高并发写的场景时，我们就可以效仿该模式。举个例子，我们需要在数据库中记录网站中不同 Banner 的点击数目，
我们就可以在缓存中记录一个计数器，合并多个结果之后再刷新到数据库中，因为 Banner 计数在多数场景中并不是十分重要的数据，丢失一些问题也不大，
所以针对类似场景，我们就可以使用该模式来提升性能。

## 缓存更新策略
无论使用以上的哪种模式，缓存更新和底层存储更新必然是两个操作，也即当收到数据更新请求时，我们必须要进行以下两个决策：

1. 先更新缓存，还是先更新底层存储？

2. 若缓存命中，是直接更新缓存的值，还是将缓存删除？

基于这两个决策，可以组合出不同的缓存更新策略，不同的策略又会有不同的潜在问题，将在本节逐一介绍。

### 并发读写场景
#### 先写缓存，再写存储
在以下场景中，线程A 执行更新操作，线程B 执行读取操作。

{{< mermaid >}}
sequenceDiagram
    participant ThreadA
    participant ThreadB
    participant Cache
    participant Data Storage

    ThreadA->>Cache: 1. 删除缓存值
    ThreadA->>Data Storage: 2. 更新数据存储值
    ThreadB->>Cache: 3. 读取缓存值未命中
    ThreadB->>Data Storage: 4. 读取底层存储中的值
    ThreadB->>Cache: 5. 将读取值写入缓存
{{< /mermaid >}}

以上是正常工作时的时序，但我们需要注意此处是并发场景，**线程之间的执行时序是无法保证的**。

因为通常 Step2 中更新底层存储的操作会相对比较耗时，此时如果 线程B 已经将旧的数据读取出来（即 Step3，Step4 先于 Step2），
并将更新前的旧值写入到缓存中，那么后续的读取操作将会一直得到旧值数据，出现了数据不一致的情况。

#### 先写存储，再写缓存
如果我们改变一下更新顺序，情况会不会好一些呢？如下所示：

{{< mermaid >}}
sequenceDiagram
    participant ThreadA
    participant ThreadB
    participant Cache
    participant Data Storage

    ThreadA->>Data Storage: 1. 更新数据存储值
    ThreadA->>Cache: 2. 删除缓存值
    ThreadB->>Cache: 3. 读取缓存值未命中
    ThreadB->>Data Storage: 4. 读取底层存储中的值
    ThreadB->>Cache: 5. 将读取值写入缓存
{{< /mermaid >}}

相对于 **先更新缓存，再更新存储** 的方式，该方式似乎表现得更好，它将耗时的底层存储更新操作放在了第一步，即使 线程B 读取到旧值写入了缓存也会被 线程A 清理掉。

但是仔细想想的话，我们会发现这种方式同样有问题，因为此处我们假设了读取请求会比更新请求执行得更快，大多数情况下确实如此，但是如果考虑 **网络情况** 的话，
则事实并不一定如此。也就是说，读取请求可能比写入请求更晚到达应用端，线程A 的写入请求已经返回并且执行了缓存清理步骤，线程B 的读取请求才返回并将旧值写入了缓存，
这种情况下仍然会存在数据不一致的问题，虽然概率很低，但仍然存在这种可能性。

**先写存储，再写缓存** 也就是模式节中的 **Cache Aside 模式**，上述的情况同样是 Cache Aside 可能存在的问题，所以通常建议：

1. 缓存失效时，读取底层存储的值后立即执行写缓存操作，降低旧值写入的概率；

2. 给所有缓存设置 **过期时间** ，可以保证即使出现不一致的情况，在一段时间后仍可恢复；

### 并发双写场景
在以下场景中，我们假设有两个线程同时进行更新操作。

{{< mermaid >}}
sequenceDiagram
    participant ThreadA
    participant ThreadB
    participant Cache
    participant Data Storage

    ThreadA->>Data Storage: 1. 更新数据为 A 值；
    ThreadA->>Cache: 2. 更新缓存值为A / 删除缓存；
    ThreadB->>Data Storage: 3. 更新数据为 B 值；
    ThreadB->>Cache: 4. 更新缓存值为B / 删除缓存；
{{< /mermaid >}}

因为我们不能保障底层数据和缓存的更新是原子操作，所以可能存在步骤穿插的情况，此时选择更新缓存，或是清理缓存就变得格外重要。

如果我们选择 **更新缓存**，虽然能确保读取时缓存始终有值，保证请求不会穿透缓存层。但是因为 **更新操作是有状态的**，我们需要一些手段来保证两个操作步骤的原子性，
否则在上述场景中，如果 Step4 先于 Step2 写入，则会出现底层存储为 B 值，但缓存中是 A 值的情况。

如果我们选择 **删除缓存**，读取请求将会穿透缓存层，但以上的双写场景是安全的。因为无论 Step2，Step4 谁先执行，缓存中都没有数据（Delete操作是**无状态、幂等**的），
读取请求都会到底层存储中请求最新数据。


{{< admonition type=tip title="Write Through模式中的问题" open=open >}}
在 **Write Through模式** 中就存在双写场景的问题，如果选择的是 **更新缓存** 的策略，则务必要保障两步写操作的原子性。
谨记：**存在状态的、非原子的双写操作，都会有数据不一致的风险**。
{{< /admonition >}}

### 缓存更新时的建议
基于上面的所有场景，总结以下几点建议：

- 缓存更新时，尽量选择 Delete 操作，而不是 Update 操作，避免状态的介入；

- 应该先更新底层存储，然后再更新缓存，可以尽量避免时序的影响；（先执行更耗时的操作）

- 为所有的缓存值设置 **过期时间**，所有的 Case 都无法保证 100% 的正确性，设置过期时间是最后的保障；

## 基于 Binlog 的缓存更新方案
在传统的 Cache Aside 模式中，我们往往是在更新数据库操作成功之后，由应用程序层将缓存删除掉，这一切都发生在应用层。这种方式有几个明显的坏处：

- 缓存删除操作可能失败，尤其当系统使用分布式缓存组件时，网络请求增加了缓存删除失败的概率；

- 为了保障数据的一致性，当缓存删除失败时，我们需要终止整个事务，或者重试缓存删除操作直至成功；

- 缓存删除的逻辑需要我们手动集成到代码中的每一个数据库更新操作，存在遗漏的风险；

如今我们有了一些用于同步数据库 Binlog 的组件，比如 Alibaba 开源的 Canal 组件，该类组件可以伪装成数据库的从节点，
监听主库的数据更新操作，并将其转换为数据更新事件，通过 Kafka 等消息队列组件通知到应用层。

(Canal的流程图)

通过启用该组件，在应用层中我们可以监听对应 kafka 的数据更新事件，然后在执行相关的缓存清理逻辑。这种方式的好处在于：

- 将缓存清理逻辑从数据库更新操作中剥离，事务不需要考虑缓存的问题，提高代码简洁度和降低遗漏概率；

- 消息队列天然具备重试的机制，只要事务执行成功，可以保障缓存也能得到清理；

{{< admonition type=warning title="低概率的数据不一致情况" open=open >}}
虽然该方案看起来已经十分优雅了，但本质上它仍然是 **Cache Aside 模式**，只是将缓存清理的操作放在了另外一个地方罢了。
所以，它仍然无法摆脱可能存在的并发读写时的数据不一致问题，即某个线程读取到某个旧值，
此时假设另外一个线程完成了更新操作，并且应用程序马上接收到更新事件，删除了缓存，前面的线程才将读取到的旧值写入缓存的情况。
所以，我们同样需要遵循缓存更新策略中提到的建议。
{{< /admonition >}}

### 一主多从的数据库架构
大多数生产环境中，关系型数据库我们会将其部署为 **一主多从架构，以此来实现读写分离**，降低主库的压力。

也就是说，应用层会有部分查询数据库的请求路由到从库中。此时如果我们只监听主库的更新就会产生问题，
因为收到主库更新事件时，即使我们将缓存删除了，但我们不能保证此时从库的数据已经跟主库保持一致了，
如果此时查询从库的请求获取到了旧的数据，又将这个旧的数据写入缓存，我们又会面临数据不一致的问题。

所以，这个问题的解法在于，我们不仅仅需要监听主库的更新，我们同样需要监听从库的更新事件。

{{< image src="https://i.bmp.ovh/imgs/2022/03/1447be7e176dc412.png" width="600" caption="数据库一主多从架构下的 Binlog 同步" >}}

在该场景中，因为我们无法精确地控制每个请求具体会调用到哪个从库，所以在所有的从库都同步到主库的进度之前，都会存在数据不一致的风险（即有线程读取了尚未同步的从库数据，将旧数据写入缓存）。
因此，我们需要监听每一个从库的 Binlog ，只要有一个从库更新了，我们就要对缓存进行清理。

假设我们是一主五从的架构，那么一个更新数据库的操作就将对应执行 **6 次**缓存清理操作，
虽然很不优雅，但为了保障一致性，似乎也没有更好的办法了。

### 集中式缓存和本地缓存
在实际项目中，除了 Redis 这一类的分布式缓存组件，有时我们也会将数据直接缓存在应用的本地内存中。
本地缓存具备更好的性能，并且可拓展性会比集中式缓存更好，常见的本地缓存如 Guava Cache 等。

本地缓存同样面临更新的问题，而且本地缓存的更新比集中式缓存稍微复杂一些，对于集中式缓存来说，只要在集群中的一个节点执行缓存清理操作即可。
但是对于本地缓存，每个节点都存在一份数据副本，为了保持一致，我们需要删除所有节点上的本地缓存数据。

综上所述，我们针对这两种缓存的处理采用以下策略：

- **集中式缓存** : 所有节点使用同一个 Kafka Consumer Group，只要其中一个节点收到数据更新事件，则清理集中式缓存的逻辑；

- **本地缓存** : 每个节点使用各自的 Kafka Consumer Group（比如使用 host 区分机器），当产生数据更新事件时，每个节点都会收到并执行清理本地缓存的逻辑；

### read-after-write场景
通过监听 Binlog 事件来删除缓存的方式比传统的手动删除方式更加通用，但是其仍然具备一些缺点，比如：

- **更新延迟** : 删除缓存的操作基于 Binlog 事件，事件要经过 MySQL -> Canal -> Kafka -> 应用程序 -> 缓存清理，整一条链路比手动删除缓存长许多，对于缓存何时会被删除，其实是没有保证的；

- **增加数据库压力** : 监听 Binlog 的组件是伪装成从节点进行工作的，所以主节点需要额外的资源来执行同步 Binlog 的任务，会对主库的性能造成一定的影响；（通常问题不大）

- **级联故障问题** : 相比于手动缓存清理操作，该方案关联的组件更多（Canal，Kafka等），任意一个节点出了问题都将导致缓存架构失效，可能会导致大规模的缓存不一致情况；

针对 **更新延迟** 的问题，如果我们系统中存在 **写后立即读** 的情况，比如下面的业务场景：

- 用户在个人页面编辑个人信息，发送请求更新页面；

- 更新数据成功之后，重新发起请求读取数据、加载页面；

这种问题就是典型的 **read-after-write** 情况，此处如果使用 Binlog 的方式去做，那么极有可能会读取到旧的数据（缓存未更新），用户会误以为更新没有成功。
在这种情况下，我们有以下几种手段解决：

1. **手动清理缓存** : 退回传统的手动缓存清理操作，更新操作完成后，使用代码直接清理缓存；

2. **避免使用缓存** : 考量应用场景，对于读取请求量不大、更新频繁的场景，尽量不实用缓存机制。缓存不是万能药，它还会给代码带来不必要的复杂性；

## Reference
- [缓存更新的套路](https://coolshell.cn/articles/17416.html)

- [并发环境下，先操作数据库还是先操作缓存？](https://juejin.cn/post/6844903907726983181)